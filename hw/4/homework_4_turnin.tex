\documentclass[a4paper]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{paralist}
\usepackage{epstopdf}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyvrb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}
\usepackage{paralist}
\usepackage[svgname]{xcolor}
\usepackage{enumerate}
\usepackage{array}
\usepackage{times}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage{environ}
\usepackage{times}
\usepackage{textcomp}
\usepackage{caption}
\usepackage[square,numbers]{natbib}



\urlstyle{rm}

\setlength\parindent{0pt} % Removes all indentation from paragraphs


% TO SHOW SOLUTIONS, include following (else comment out):
\newenvironment{soln}{
    \leavevmode\color{blue}\ignorespaces
}{}


\hypersetup{
%    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\geometry{
  top=1in,            % <-- you want to adjust this
  inner=1in,
  outer=1in,
  bottom=1in,
  headheight=3em,       % <-- and this
  headsep=2em,          % <-- and this
  footskip=3em,
}


\title{Assignment 4} % Title
\author{
Derek Paulsen \\
} 

\date{}



\newcommand{\ind}[1]{\mathds{1}[#1]}
\newcommand{\indeg}[1]{\delta_{in}(#1)}
\newcommand{\outdeg}[1]{\delta_{out}(#1)}
\def \eps{\epsilon}
\begin{document}
\maketitle 

\section{Problem 1}
\subsection{Part A}
The first constraint has that all nodes
have a flow conservation constraint, i.e. each node must have net zero flow. 
the second constraint states that the total flow over all edges must sum to 1. 
Now consider a feasible solution to this LP. Since we must have positive flow 
on some of the edges and every node has a flow conservation constraint, any feasible 
solution must be a circulation to be feasible. 

As the hint suggests. We can decompose any circulation into a set of cycles each with non-zero flow. 
Assume for contradiction that we have a \textit{unique} optimal solution $x^*$ which is composed of 
cycles $\{C_1, ..., C_k\}$, each contributing a total of $\{h_1, ..., h_k\}$ flow to the solution for some $k > 1$.

$$
h_j = \sum_{(u,v)\in C_j} x_{u,v}
$$ 

The cost of each of the cycles is,
$$
\sum_{(u,v)\in C_j} \frac{c(u,v)}{ |C_j| } \times h_j
$$

WLOG we assume that $C_1$ is the cycle with the minimum average cost. We then have 
$$
\sum_{(u,v)\in C_1} \frac{c(u,v)}{ |C_1| } \leq  \sum_{(u,v)\in C_j} \frac{c(u,v)}{ |C_j| }, \quad \forall j=2,...,k
$$

Since all flows are positive we then have, 
$$
\sum_{(u,v)\in C_1} \frac{c(u,v)}{ |C_1| } \times h_j \leq  \sum_{(u,v)\in C_j} \frac{c(u,v)}{ |C_j| } \times h_j , \quad \forall j=2,...,k
$$


This implies that we could obtain a solution with cost less than or equal to $x^*$, 
by pushing $\sum_{i=2}^k h_i / |C_1|$ flow around cycle $C_1$ and still have a 
feasible solution. This contradicts our assumption that $x^*$ is a unique optimal 
solution, therefore there exists an optimal solution which only contains a single cycle. 
The cost of this optimal solution is then 
$$
\sum_{(u,v)\in C_j} \frac{c(u,v)}{ |C_j| } \times 1
$$

Which is just the value of the minimum mean cost cycle. This completes the proof.


\subsection{Part B}
We first rewrite the LP in standard notation for flow LP's.

Let 
$$
A \in \mathds{R}^{|V| \times |E|} = \begin{cases} -1 \text{ if } (i,j) \in E\\ 1 \text{ if } (j,i) \in E\\ 0 \text{ otherwise}\end{cases}
$$

We can then write our LP as, 
\begin{align*}
	\min_x \quad &c^Tx\\
	\text{subject to} \quad Ax &= 0\\
		   \mathds{1}^Tx &= 1\\
						x &\geq 0\\
\end{align*}

We can then write down the dual,

\begin{align*}
	\max_{p, \epsilon} \quad &\epsilon\\
	\text{subject to} \quad p^TA + \mathds{1}^T\epsilon \leq c^T\\
\end{align*}


We now note that the dual defines a potential $p$, which maximizes 
the the minimum of $c(u,v) + p(u) - p(v)$ 

\begin{align*}
	p^TA + \mathds{1}^T \epsilon &\leq c^T\\
	\mathds{1}^T \epsilon &\leq c^T - p^TA\\
\end{align*}

Which implies, 

$$
\epsilon = \min_{(u,v) \in E} c(u,v) + p(u) - p(v) = \min_{(u,v) \in E} c_p(u,v)
$$

Which is exactly the definition of $\epsilon$ from class,
this completes the proof.


\subsection{Part C}
From strong duality, we have that for primal and dual optimal soluions $x^*$ and $p^*, \epsilon^*$,

$$
c^Tx^* = \epsilon^*
$$

Since, the primal's optimal cost is the mimimum mean cost cycle, this means that 
$\epsilon$ is also equal to the mimimum mean cost cycle. 

\section{Problem 2}

Consider the following LP
\begin{align*}
	\max_\pi \quad & \pi^T 0 \\
	\text{subject to} \quad \pi^TP &= \pi^T\\
		   \pi^T\mathds{1} &= 1\\
						\pi &\geq 0\\
\end{align*}

We can equivalently rewrite this as, 
\begin{align*}
	\max_\pi \quad & \pi^T 0 \\
	\text{subject to} \quad \pi^T(P - I) &= 0\\
		   \pi^T\mathds{1} &= 1\\
						\pi &\geq 0\\
\end{align*}

Now consider the dual, 

\begin{align*}
	\min_{x, y} \quad &  0^Tx + y  \\
	\text{subject to} \quad (P - I)x + \mathds{1}y \geq  0\\
\end{align*}

Because the rows of $P$ sum to 1, and all entries are non-negative, we have that 

$$
Px \leq \max_{i = 1,...,n} x_i
$$

This implies that for some element of $(P-I)x$ is $\leq 0$ because $(P-I)x = Px - x$. 
By the constraint of dual $(P - I)x + \mathds{1}y \geq  0$, this 
implies that $y \geq 0$. meaning that the problem is bounded. Additionally, 
there is a trivial feasible solution to the problem of $x = (0,...,0), y = 0$. 
Because the dual is bounded and feasible, this implies that it has a optimal solution, 
by strong duality we have that the primal must also have an optimal solution. Finally,
this implies that there exists $\pi$ that statifies that contraints of the primal, since 
$P$ was arbitrary we have that 
$$
\forall \text{ stochastic matrices } P, \exists \pi\quad  \pi^TP = \pi^T \land \sum_{i=1}^{n} \pi_i = 1
$$

Which is the statement we wanted to prove.

\section{Problem 3}

Consider the following LP,
\begin{align*}
	\min_x \quad  \max_{y,z} x^TAy + &x^TBz\\
	\text{subject to} \quad x,y,z &\in \Delta_p
\end{align*}

We then note that the objective is equalvalent to minimizing the sum 
of the largest element of $x^TA$ and $x^TB$, since the player 2 and 3 will 
just choose to play the strategy which takes the max of these elements.

We can introduce auxillary variables to rewrite, 

\begin{align*}
	\min_{x, v_1, v_2} \quad  v_1 + v_2\\
	\text{subject to} \quad x^TA &\leq \mathds{1}^Tv_1\\
							x^TB &\leq \mathds{1}^Tv_2\\
							\sum_{i=1}^nx_i &=1 \\
							x &\geq 0\\
\end{align*}

Rearrange, 

\begin{align*}
	\min_{x, v_1, v_2} \quad  v_1 + v_2\\
	\text{subject to} \quad x^TA - v_1 &\leq 0 \\
							x^TB - v_2 &\leq 0 \\
							\sum_{i=1}^nx_i &=1 \\
							x &\geq 0\\
\end{align*}

\begin{align*}
	\min_{x, v_1, v_2} \quad  v_1 + v_2\\
	\text{subject to} \quad -x^TA + v_1 &\geq 0 \\
							-x^TB + v_2 &\geq 0 \\
							\sum_{i=1}^nx_i &=1 \\
							x &\geq 0\\
\end{align*}



We then take the dual and get, 
\begin{align*}
	\max_{y,z,v_3} \quad  v_3\\
	\text{subject to} \quad -Ay - Bz + v_3 &\leq 0 \\
							\sum_{i=1}^ny_i &=1 \\
							\sum_{i=1}^nz_i &=1 \\
							y,z &\geq 0\\
\end{align*}

simplify 

\begin{align*}
	\max_{y,z,v_3} \quad  v_3\\
	\text{subject to} \quad Ay + Bz  &\geq v_3 \\
							\sum_{i=1}^ny_i &=1 \\
							\sum_{i=1}^nz_i &=1 \\
							y,z &\geq 0\\
\end{align*}

We now note however that this is equivalent to, 
\begin{align*}
	\max_{y,z} \quad \min_{x} x^TAy + &x^TBz\\
	\text{subject to} \quad x,y,z &\in \Delta_p
\end{align*}


By strong duality we then have that the above dual has 
the same optimal value as the primal, 

\begin{align*}
	\min_x \quad  \max_{y,z} x^TAy + &x^TBz\\
	\text{subject to} \quad x,y,z &\in \Delta_p
\end{align*}

Furthermore, both the dual and primal have trivial feasible solutions
which are any combination of the unit vectors in $\mathds{R}^n$. By strong duality 
since both problems are feasible they both have an optimal solution which is the same 
value, which is the definition of a Nash equilibrium. This completes the proof.




\section{Problem 4}
\subsection{Part A}
We note that the constraints of the LP relaxation are simply a statement of Hall's Theorem, 
however there is an equivalent formulation of this LP which has fewer constraints, which is as follows,

Let $y_{i,j,t}$ be the fraction of time that client $i$ stays are property $j$ during 
time slot $t$. 
Let $s_{i,j,t}$ be the preference for client $i$ for property $j$ during 
time slot $t$, which is 0 if client doesn't want to stay at the property or 1 if they do. 
We can then construct the LP as follows, 

\begin{align}
	\min_{x} \sum_{j=1}^m x_j\\
	\text{subject to}\\
	\sum_{i=1}^n y_{i,j,t} &\leq x_j \quad \forall j\in [m], t \in [T]\\
	y_{i,j,t} &\leq s_{i,j,t} \quad \forall i\in [n], j\in [m], t \in [T]\\
	\sum_{j=1}^m y_{i,j,t} &= 1 \quad \forall i\in [n], t \in [T]\\
		0 \leq x &\leq 1\\
		0 \leq y &\leq 1
\end{align}

The above LP has a polynomial number of constraints, hence we can easily apply the 
ellipsoid method to get a polynomial runtime to find a solution.

\subsection{Part B}

Consider the set of $k$ clients that are not yet matched for some week $t$, from the constraints of 
the LP we have that,
$$\sum_{j\in \bigcup_{i \ in S} S_{i,t}} x_j \geq |S|$$

Let $N$ be the set of clients which are reachable via an alternating path
in the residual graph of the current matching starting from an unmatched client.
Again from the constraints we have that

$$\sum_{j\in \bigcup_{i \ in N} S_{i,t}} x_j \geq |N|$$

If we then remove all the locations currently used in the matching we get all 
of locations, which if they were added, would increase the size of the matching 
by 1. We now note that there are $|N| - k$ locations used in the current matching, each 
corresponding $x_j$ is at most 1 hence we get, 
$$\sum_{j\in \bigcup_{i \ in N} S_{i,t} \setminus \{\text{properties used in the matching}\}} x_j \geq |N| - |N| + k$$
$$\sum_{j\in \bigcup_{i \ in N} S_{i,t} \setminus \{\text{properties used in the matching}\}} x_j \geq k$$

By construction the probability that one of these properties is bought is,
$$ (\sum_{j\in \bigcup_{i \ in N} S_{i,t}  \setminus \{\text{properties used in the matching}\}} x_j) / OPT_{LP} \geq  k / OPT_{LP}$$
Therefore the probability that the matching increases by one is at least $k / OPT_{LP}$.

\subsection{Part C}

We begin with the probability that client $i$ is matched for a partiucular round, 

$$Pr[\text{client $i$ is matched for week $t$ on round $r$}] \geq 1/OPT_{LP}$$

We then get, 
$$Pr[\text{client $i$ is not matched for week $t$ on round $r$}] \leq (1 - 1/OPT_{LP})$$

Which means that after $r$ rounds of adding properties we have,

$$Pr[\text{client $i$ is unmatched for week $t$ after $r$ rounds}] \leq (1 - 1/OPT_{LP})^r$$

We can then union bound to get the probability that the client is unmatched for any week

$$Pr[\text{client $i$ is unmatched for any week after $r$ rounds}] \leq T(1 - 1/OPT_{LP})^r$$

Finally, we union bound one more time to get probability that any client in unmatched, 

$$Pr[\text{any client is unmatched for any week after $r$ rounds}] \leq nT(1 - 1/OPT_{LP})^r$$

We can then derive a value of $r$ that will give us a solution with high probability, 

\begin{align*}
	nT(1 - 1/OPT_{LP})^r \leq nT e^{-r/OPT_{LP}}\\
\end{align*}

If we set $r = 4 OPT_{LP}\log n \log T$, we then compute the probability that 
any client is unmatched, for the first case, if $n > T$, 
\begin{align*}
	nT e^{-r/OPT_{LP}} &= nT \frac{1}{n^{4\log T}}\\
					&\leq n^2 \frac{1}{n^{4\log T}}\\
					&\leq  \frac{1}{n^{4\log T - 2}}\\
					&\leq  \frac{1}{n^{4\log T - 2}}\\
					&\leq  \frac{1}{n^2}\\
\end{align*}

If $T > n$,

\begin{align*}
	nT e^{-r/OPT_{LP}} &= nT \frac{1}{T^{4\log n}}\\
					&\leq T^2 \frac{1}{T^{4\log n}}\\
					&\leq  \frac{1}{T^2}\\
\end{align*}

We now note that, $r = 4OPT_{LP} \log n \log T \leq 4OPT\log n \log T$.

Hence we get our final algorithm which solves the LP relaxation in polynomial time, 
then random adds properties until we have perfect matchings for all weeks. We note that checking 
perfect matchings for each week is polynomial time using any maximum matching algorithm. Finally, we have shown that we 
terminate with high probability after $4OPT\log n \log T = O(OPT\log n \log T)$ rounds, hence 
our solution is $O(\log n  \log T)$ approximate and runs in polynomial time.

\end{document}
